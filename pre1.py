# -*- coding: utf-8 -*-
"""pre1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UfFStzcvr409tkb8LL0Q1P5WBClSkvvD

Divar is an  Online Requirements Platform in Iran. Amirkabir university in Iran Held a contest with  title "Intelligent processing of Divar data'. In this contest, participants must predict what percentage of an ad will be able to be published on the Divar website. So it is a NLP question with Persian text. Hazm library has been provided for this purpose.

First, I install hazm library and some other requirements.
Hazm Repo in github. Hazm Repo in github. Check this link [https://github.com/sobhe/hazm]Hazm-GitHub for more information.
"""

pip install hazm

pip install arabic_reshaper

pip install python-bidi

"""I import some usable libraries such as pandas, numpy,etc."""

# Commented out IPython magic to ensure Python compatibility.
# data analysis pkg
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import numpy as np

# data wrangling, cleaning and wordcloud visualization pkg
from hazm import word_tokenize, stopwords_list, InformalLemmatizer
import re
import arabic_reshaper
from bidi.algorithm import get_display
from wordcloud import WordCloud

from tqdm import tqdm as progressbar

# Avoiding warnings
import warnings
import os
########### Prevent Warnings ###########
warnings.filterwarnings(action='ignore')
########### Prevent Warnings ###########

# %matplotlib inline

"""I wrote my codes in GoogleColab. I connect to the colab drive.

---


"""

from google.colab import drive
drive.mount('/content/drive/')

"""The data has been given in parquet form so I import pyarrow.parquet to read the data but I have uploaded it in the google drive.


"Parquet is a columnar storage format that supports nested data.

Parquet metadata is encoded using Apache Thrift.

The Parquet-format project contains all Thrift definitions that are necessary to create readers and writers for Parquet files."
https://github.com/apache/parquet-format
"""

import pyarrow.parquet as pq
df = pq.read_table(source="/content/drive/MyDrive/DivarNLP/DMC-Train.parquet").to_pandas()

df

"""As you see we have four columns, "post_id","post_data","review_label","reject_reason_id". The last column is for more information about the reason of acceptance or rejection of ads.

Relationship between Features and Survival
In this section, we analyze relationship between different features with respect to reviw_label. We see how different feature values show different acceptance chance. We also plot different kinds of diagrams to visualize our data and findings.

In review_label I replace 'accept' with 1 and 'reject' with 0.
"""

Acceptance= df.review_label.replace('accept',1,inplace=True)
Not_Acceptance= df.review_label.replace('reject',0,inplace=True)

"""Now we compute how many 1 and 0 we have in review_label column."""

df['review_label'].value_counts()

# print ("review_label: %i (%.1f%%)"%(len(Acceptance), float(len(Acceptance))/len(df)*100.0))
# print ("Not_Acceptance: %i (%.1f%%)"%(len(Not_Acceptance), float(len(Not_Acceptance))/len(df)*100.0))
# print ("Total: %i"%len(df))

df.review_label = df.review_label.astype('int')
sns.countplot(df.review_label)

"""As we can see our labels are unbalanced so in the following we should keep in mind to balance them."""

df.head(10)

"""Values of column 'post_data' are in dictionary form. So first we import json to conver dictionary string to dictionary and see the 0 ondex of this column. 

"""

# convert dictionary string to dictionary
# using json.loads()
import json 
# using json.loads()
# convert dictionary string to dictionary
a=df.post_data[0]
res = json.loads(a)
  
# print result
str(res)

"""Then we merge the columns which has been added by dictionary to the previous column. We use pd.concat() . It takes some time.

---


"""

import json 
import pandas as pd
dff=pd.concat([df, df.post_data.apply(json.loads).apply(pd.Series)], axis=1)

"""We copy the dataframe dff as dataa"""

dataa=dff.copy()

"""Transform "dataa" to a csv file"""

dataa.to_csv('dataa.csv', index=False)

"""Now read the dataa.csv"""

dffff=pd.read_csv("dataa.csv")

"""Save dataa.csv to our drive."""

!cp dataa.csv "drive/My Drive/"

"""Let's take a look to our dataframe 'dffff'

# Looking into the training dataset
Printing first 5 rows of the train dataset.


"""

dffff.head(5)

dffff.columns

"""Below is a brief information about each columns of the dataset:

1.   'post_id':Unique id of the post
2.   'post_data
3.   'review_label'
4.   'reject_reason_id'
5.   'body_status'
6.   'brand':english and persian name of the brand separated with ::(two colons)
7.   'brand_model'
8.   'category'
10.  'color'
11.  'description'
12.  'document'
13.  'gearbox'
14.  'new_price':Price of the post in Iranian Toman.
15.  'post_type'
16.  'selling_type'
17.  'third_party_insurance_deadline'
18.  'title':Title of the post. 
19.  'usage'
20.   'year'
21.   'options'

Total rows and columns

We can see that there are 540362 rows and 20 columns in our training dataset.
"""

dffff.shape

"""We use info() method to see more information of our train dataset.


"""

dffff.info()

"""We can see that ..... value is missing for many rows.

Out of ....  rows, the value is present only in .... rows.

Similarly, Cabin values are also missing in many rows. Only ... out of ... rows have Cabin values.

Describing training dataset

describe() method can show different values like count, mean, standard deviation, etc. of numeric data types.
"""

dffff.describe()

"""describe(include = ['O']) will show the descriptive statistics of object data types.


"""

dffff.describe(include=['O'])

"""This shows that there are ..."""

dffff.isnull().sum()

"""There are ... rows with missing ....., ... rows with missing ...... and ... rows with missing Embarked information.

As we see wee have 20 columns with text value and number value.
Let's start with 'color' column. In this column we replace unwanted string '\u200c' with empty space.
"""

dffff['color'] = dffff['color'].str.replace('\u200c','')

dffff['color']

"""I replace the text value of column 'color' which are the name of colors to the number value. I do not use OneHotEncoder cause the number of columns are alot and OneHotEncoder makes sparse problem.

---


"""

dffff['color'].replace(['سفید', 'خاکستری', 'سفید صدفی', 'آبی', 'دلفینی', 'نوکمدادی', 'زرد',
       'نقرهای', 'مشکی', 'سرمهای', 'یشمی', 'سبز', 'طوسی', 'عنابی',
       'اطلسی', 'نقرآبی', 'بادمجانی', 'قهوهای', 'بنفش', 'قرمز', 'بژ',
       'مسی', 'زرشکی', 'نارنجی', 'آلبالویی', 'کرم', 'ذغالی', 'کربنبلک',
       'موکا', 'طلایی', 'پوستپیازی', 'زیتونی', 'برنز', 'عدسی', 'سربی',
       'خاکی', 'تیتانیوم', 'گیلاسی'],  
       [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38],inplace=True)

"""Fill nan value with zero."""

dffff["color"]=dffff["color"].fillna(0)

"""At the end change the type of values to integers."""

dffff['color'] = dffff['color'].astype(int)

"""The next column which we want to consider is "selling_type" column. Let's see what are the values of this column."""

dffff['selling_type'].unique()

"""As we see this column has four values, "cash", "cash_instalment" and "instalment" and nan values. For filling the nan values we use value_counts and fill them with the most common value."""

dffff['selling_type'].value_counts()

"""We replace this text values with numbers and nan value with the number of most prevalent one."""

dffff["selling_type"]=dffff["selling_type"].fillna(2)

dffff["selling_type"] = dffff["selling_type"].replace({"cash": 1,"cash-instalment":2,"instalment":3})

# dffff[dffff['year'] == '۱۳۶۶'].index.values

# dffff.drop(dffff.index[dffff['year'] == 'قبل از ۱۳۶۶'], inplace = True)

# dffff['year'] = dffff['year'].replace({'nan':1360})

"""The 'year' column is the next to have a look. If we want to see tha nan index in this column we write the following code.

"""

dffff[dffff['year'] == 'nan'].index.values

"""Now I replace the value which has both text and number as just the number"""

dffff['year'] = dffff['year'].replace({'قبل از ۱۳۶۶':1366})

"""The numbers of column 'color' are in percian format so I replace the one number which is in English format """

dffff['year'] = dffff['year'].replace({'1366':'۱۳۶۶'})

dffff['year']=dffff['year'].map({1366:'۱۳۶۶','۱۳۹۶':'۱۳۹۶','۱۴۰۰':'۱۴۰۰','۱۳۸۶':'۱۳۸۶','۱۳۸۱':'۱۳۸۱','۱۳۸۷':'۱۳۸۷','۱۳۸۴':'۱۳۸۴',
                                    '۱۳۹۷': '۱۳۹۷','۱۳۹۰':'۱۳۹۰','۱۳۸۹':'۱۳۸۹','۱۳۸۳':'۱۳۸۳','۱۳۹۵':'۱۳۹۵','۱۳۹۹':'۱۳۹۹','۱۳۹۴':'۱۳۹۴','۱۳۹۸':'۱۳۹۸',
                                   '۱۳۹۳':'۱۳۹۳','۱۳۹۲':'۱۳۹۲','۱۳۷۹':'۱۳۷۹','۱۳۹۱':'۱۳۹۱','۱۳۷۶':'۱۳۷۶','۱۳۷۷':'۱۳۷۷','۱۳۷۸':'۱۳۷۸','۱۳۷۱':'۱۳۷۱',
                                  '۱۳۸۵':'۱۳۸۵','۱۳۸۲':'۱۳۸۲', '۱۳۸۸':'۱۳۸۸','۱۳۸۰':'۱۳۸۰','۱۳۶۶':'۱۳۶۶','۱۳۷۰':'۱۳۷۰','۱۳۷۵':'۱۳۷۵','۱۳۶۷':'۱۳۶۷',
                                 '۱۳۷۳':'۱۳۷۳','۱۳۷۲':'۱۳۷۲', '۱۳۷۴':'۱۳۷۴','۱۳۶۹':'۱۳۶۹','۱۳۶۸':'۱۳۶۸','1360':'۱۳۶۰' ,'nan':'۱۳۶۰'})

dffff['year'] = dffff['year'].replace({1360:'۱۳۶۰'})

dffff['year']=dffff['year'].apply(str)

dffff['year'] = dffff['year'].astype(int)

dffff[dffff['year'] == 'nan'].index.values

dffff['year'].unique()

dffff.shape

"""About column "third_party_insurance_deadline" first we check the unique values of this column and then fill the null values."""

dffff["third_party_insurance_deadline"].unique()

dffff["third_party_insurance_deadline"]=dffff["third_party_insurance_deadline"].fillna(0)

dffff['third_party_insurance_deadline'] = dffff['third_party_insurance_deadline'].astype(int)

"""I replace the text value of column 'brand' which are the name of brands to the number value. I do not use OneHotEncoder cause the number of columns are alot and OneHotEncoder makes sparse problem."""

dffff["brand"] = dffff["brand"].replace({'تیبا::Tiba': 1})

dffff["brand"] = dffff["brand"].replace({'لیفان::Lifan': 2, 'پژو ۴۰۵::Peugeot 405':3, 
'پژو ۲۰۶\u200d صندوق\u200cدار::Peugeot 206':4, 'نیسان::Nissan':5, 'پژو ۲۰۷::Peugeot 207':6, 'پژو پارس::Peugeot Pars':7, 
 'پراید هاچ\u200cبک::Pride':8,  'پیکان::Peykan':9, 'پژو ۲۰۶\u200d::Peugeot 206':10,  'پراید صندوق\u200cدار::Pride':11, 'ام\u200cوی\u200cام::MVM':12,
  'زانتیا::Citroen Xantia':13, 'مزدا::Mazda':14,  'کیا::Kia':15, 'رنو::Renault':16, 'سمند::Samand':17, 'تندر ۹۰::Tondar 90':18,
   'تویوتا::Toyota':19,  'هیوندای (غیره)::Hyundai':20, 'رانا::Runna':21, 'هیوندای آزرا::Hyundai Azera':22, 'بی\u200cام\u200cو::BMW':23,
    'هیوندای سوناتا::Hyundai Sonata':24,  'بنز::Mercedes-Benz':25, 'دوو::Daewoo':26,
                                      'گل::Gol' :27, 'پژو روآ / آر\u200cدی::RD/ROA':28})

dffff["brand"] = dffff["brand"].replace(to_replace ="وانت",
                 value =29)

dffff["brand"] = dffff["brand"].replace(to_replace ="سایر",
                 value =30)

dffff["brand"]=dffff["brand"].fillna(0)

dffff['brand'] = dffff['brand'].astype(int)

"""The same thing happen to "body_status" column."""

dffff["body_status"] = dffff["body_status"].replace({'witout-color': 1, 'intact':2, 'few-spots-of-color':3,  'multiple-spots-paint':4, 
                                                  'half-paint' :5, 'two-spots-of-color':6, 'one-spot-paint':7, 'two-spots-paint':8, 'some-scratches':9,
                                                    'full-paint':10, 'one-spot-of-color':11,  'by-accident':12, 'out':13,'paintless-dent-removal':14,
                                              'accidental'     :15, 'junk':16})

dffff["body_status"]=dffff["body_status"].fillna(0)

dffff['body_status'] = dffff['body_status'].astype(int)

dffff.head(3)

dffff["gearbox"].unique()

dffff["gearbox"].value_counts()

dffff["gearbox"] = dffff["gearbox"].replace({"automatic": 0,"manual":1})

dffff["gearbox"]=dffff["gearbox"].fillna(0)

dffff.gearbox = dffff.gearbox.astype('int')
sns.countplot(dffff.gearbox)

import re # for regex
def clean(text):
    cleaned = re.compile(r'\n')
    return re.sub(cleaned,'',text)

dffff.description = dffff.description.apply(clean)

def is_special(text):
    rem = ''
    for i in text:
        if i.isalnum():
            rem = rem + i
        else:
            rem = rem + ' '
    return rem

dffff.description = dffff.description.apply(is_special)

import re # for regex
def clean5(text):
    cleaned = re.compile(r'\\u')
    return re.sub(cleaned,'',text)

dffff.description = dffff.description.apply(clean5)

dffff['text'] = dffff['title'] + " "+ dffff['description']

def set_types(df):
    df.title = df.title.astype(str)
    df.description = df.description.astype(str)
    return df

dffff=set_types(dffff)

def replace_nan(entry):
    if entry == 'nan':
        return '#'
    return entry
dffff.title = dffff.title.apply(replace_nan)
dffff.description = dffff.description.apply(replace_nan)

# dffff.drop_duplicates(subset =['description', 'title'], inplace = True)

lemma = InformalLemmatizer()

# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b
def remove_emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)

# https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py  emoticons list
# https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt Chat shortcuts

def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r' ', text)

def remove_punctuations(text):
    punctuations = re.compile(r'[~`!@#$%^&*(,<،>){}\\/|\'"?؟_+-=~\[\]]')
    return punctuations.sub(r' ', text)

def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r' ', text)

def remove_weird_chars(text):
    weridPatterns = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u'\U00010000-\U0010ffff'
                               u"\u200d"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\u3030"
                               u"\ufe0f"
                               u"\u2069"
                               u"\u2066"
                               u"\u200c"
                               u"\u2068"
                               u"\u2067"
                               "]+", flags=re.UNICODE)
    patterns = [re.compile('\r'), re.compile('\n'), re.compile('&amp;')]
    text = weridPatterns.sub(r'', text)
    for p in patterns:
        text = p.sub(r' ', text)
    return text

def remove_extra_repeated_alpha(text):
    """
    Remove extra repeated alphabets in a word
    check these links:
    demo : https://regex101.com/r/ALxocA/1
    Question: https://bit.ly/2DoiPqS
    """
    return re.sub(r'([^\W\d_])\1{2,}', r'\1', text)


def clean_up(text, url=True, html=True, weird_patterns=True , lemmatize=False, 
               stopwords=True, isalpha=False, punctuations=True, remove_extra_alpha=True):
    # remove url
    if url:
        text = remove_urls(text)
    # remove html tags
    if html:
        text = remove_html(text)
    # remove emokis / symbols & pictographs / transport & map symbols / flags (iOS)
    if weird_patterns:
        text = remove_weird_chars(text)
    # remove punctuations
    if punctuations:
        text = remove_punctuations(text)
    # Alter words with repeated alphabets
    if remove_extra_repeated_alpha:
        text = remove_extra_repeated_alpha(text)
    # tokenize text
    tokens = word_tokenize(text)
    # remove stop words
    if stopwords:
        tokens = [word for word in tokens if word not in stopwords_list()]
    # remove non-alphabetic items
    if isalpha:
        tokens = [word for word in tokens if word.isalpha()]
    # lemmatize words
    if lemmatize:
        tokens = [lemma.lemmatize(word) for word in tokens]
    text = ' '.join(tokens)
    
    return text

dffff = set_types(dffff)
dffff.text = dffff.text.apply(str)

all_words_title = ' '.join([text for text in dffff.title])
all_words = arabic_reshaper.reshape(all_words_title)

"""Since the dataset is not too large, we can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the corr() method
Now let’s look at how much each attribute correlates with the median house value:
"""

corr_matrix=dffff.corr()
corr_matrix['review_label'].sort_values(ascending=False)

"""The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation; for example, the median reviewlabel tends to go up when the usage goes up. When the coefficient is close to –1, it means that there is a strong negative correlation; you can see a small negative correlation review_label between the reject_reason_id and the review_label. Finally, coefficients close to 0 mean that there is no
linear correlation.

Correlating Features
Heatmap of Correlation between different features:

Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.

Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.

In our case, we focus on which features have strong positive or negative correlation with the Survived feature.
"""

plt.figure(figsize=(15,6))
sns.heatmap(dffff.drop('post_id',axis=1).corr(), vmax=0.6, square=True, annot=True)

"""We delete some columns which I think are not important.

# Feature Extraction
In this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form.
"""





dffff=dffff.drop("brand_model",axis=1)

dffff=dffff.drop(["post_data","post_type","category","document","usage"],axis=1)

dffff=dffff.drop('options',axis=1)

dffff=dffff.drop('new_price',axis=1)

dffff.to_csv('dffff.csv', index=False)

pd.read_csv('dffff.csv')

from google.colab import drive
drive.mount('drive')

!cp dffff.csv "drive/My Drive/"

!git init

!git clone https://github.com/Didehkhani/DivardataNLP.git

! pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd DivardataNLP/

! git remote -v

! git status

! git add -A

! git status

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Github/

