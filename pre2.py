# -*- coding: utf-8 -*-
"""pre2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LKd06lzPbZnRVyHDqrKv8p0XSUCbYHh5
"""

pip install hazm

pip install arabic_reshaper

pip install python-bidi

# Commented out IPython magic to ensure Python compatibility.
# data analysis pkg
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import numpy as np

# data wrangling, cleaning and wordcloud visualization pkg
from hazm import word_tokenize, stopwords_list, InformalLemmatizer
import re
import arabic_reshaper
from bidi.algorithm import get_display
from wordcloud import WordCloud

from tqdm import tqdm as progressbar

# Avoiding warnings
import warnings
import os
########### Prevent Warnings ###########
warnings.filterwarnings(action='ignore')
########### Prevent Warnings ###########

# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive/')

import pyarrow.parquet as pq

dfv = pq.read_table(source="/content/drive/MyDrive/DivarNLP/DMC-Phase1-Validation.parquet").to_pandas()

dfv

# convert dictionary string to dictionary
# using json.loads()
import json 
# using json.loads()
# convert dictionary string to dictionary
b=dfv.post_data[0]
res = json.loads(b)
  
# print result
str(res)

import pandas as pd
dffv=pd.concat([dfv, dfv.post_data.apply(json.loads).apply(pd.Series)], axis=1)

dffv

dataav=dffv.copy()

dataav.to_csv('dataav.csv', index=False)

dffffv=pd.read_csv("dataav.csv")

!cp dataav.csv "drive/My Drive/"

dffffv

dffffv.shape

dffffv['color'] = dffffv['color'].str.replace('\u200c','')

dffffv['color'].replace(['سفید', 'خاکستری', 'سفید صدفی', 'آبی', 'دلفینی', 'نوکمدادی', 'زرد',
       'نقرهای', 'مشکی', 'سرمهای', 'یشمی', 'سبز', 'طوسی', 'عنابی',
       'اطلسی', 'نقرآبی', 'بادمجانی', 'قهوهای', 'بنفش', 'قرمز', 'بژ',
       'مسی', 'زرشکی', 'نارنجی', 'آلبالویی', 'کرم', 'ذغالی', 'کربنبلک',
       'موکا', 'طلایی', 'پوستپیازی', 'زیتونی', 'برنز', 'عدسی', 'سربی',
       'خاکی', 'تیتانیوم', 'گیلاسی'],  
       [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38],inplace=True)

dffffv["color"]=dffffv["color"].fillna(0)

dffffv['color'] = dffffv['color'].astype(int)

dffffv.shape

dffffv["selling_type"]=dffffv["selling_type"].fillna(2)

dffffv["selling_type"] = dffffv["selling_type"].replace({"cash": 1,"cash-instalment":2,"instalment":3})

dffffv.shape

dffffv['year'].unique()

dffffv['year'].value_counts()

dffffv[dffffv['year'] == '۱۳۶۶'].index.values

dffffv[dffffv['year'] == 'nan'].index.values

# dffffv.drop(dffffv.index[dffffv['year'] == 'قبل از ۱۳۶۶'], inplace = True)

dffffv['year'] = dffffv['year'].replace({'nan':1360})

dffffv['year'] = dffffv['year'].replace({'قبل از ۱۳۶۶':1366})

dffffv['year'].unique()

type(dffffv['year'][10])

dffffv[dffffv['year']=='nan'].index.values

dffffv['year'] = dffffv['year'].replace({'1366':'۱۳۶۶'})

dffffv['year'].unique()

dffffv['year']=dffffv['year'].map({1366:'۱۳۶۶','۱۳۹۶':'۱۳۹۶','۱۴۰۰':'۱۴۰۰','۱۳۸۶':'۱۳۸۶','۱۳۸۱':'۱۳۸۱','۱۳۸۷':'۱۳۸۷','۱۳۸۴':'۱۳۸۴',
                                    '۱۳۹۷': '۱۳۹۷','۱۳۹۰':'۱۳۹۰','۱۳۸۹':'۱۳۸۹','۱۳۸۳':'۱۳۸۳','۱۳۹۵':'۱۳۹۵','۱۳۹۹':'۱۳۹۹','۱۳۹۴':'۱۳۹۴','۱۳۹۸':'۱۳۹۸',
                                   '۱۳۹۳':'۱۳۹۳','۱۳۹۲':'۱۳۹۲','۱۳۷۹':'۱۳۷۹','۱۳۹۱':'۱۳۹۱','۱۳۷۶':'۱۳۷۶','۱۳۷۷':'۱۳۷۷','۱۳۷۸':'۱۳۷۸','۱۳۷۱':'۱۳۷۱',
                                  '۱۳۸۵':'۱۳۸۵','۱۳۸۲':'۱۳۸۲', '۱۳۸۸':'۱۳۸۸','۱۳۸۰':'۱۳۸۰','۱۳۶۶':'۱۳۶۶','۱۳۷۰':'۱۳۷۰','۱۳۷۵':'۱۳۷۵','۱۳۶۷':'۱۳۶۷',
                                 '۱۳۷۳':'۱۳۷۳','۱۳۷۲':'۱۳۷۲', '۱۳۷۴':'۱۳۷۴','۱۳۶۹':'۱۳۶۹','۱۳۶۸':'۱۳۶۸','1360':'۱۳۶۰' })

dffffv['year'] = dffffv['year'].replace({'nan':'۱۳۶۰'})

dffffv['year'].unique()

dffffv['year']=dffffv['year'].apply(str)

dffffv['year'] = dffffv['year'].astype(int)

dffffv.shape

dffffv["third_party_insurance_deadline"]=dffffv["third_party_insurance_deadline"].fillna(0)

dffffv['third_party_insurance_deadline'] = dffffv['third_party_insurance_deadline'].astype(int)

dffffv.shape

dffffv["brand"] = dffffv["brand"].replace({'تیبا::Tiba': 1})

dffffv["brand"] = dffffv["brand"].replace({'لیفان::Lifan': 2, 'پژو ۴۰۵::Peugeot 405':3, 
'پژو ۲۰۶\u200d صندوق\u200cدار::Peugeot 206':4, 'نیسان::Nissan':5, 'پژو ۲۰۷::Peugeot 207':6, 'پژو پارس::Peugeot Pars':7, 
 'پراید هاچ\u200cبک::Pride':8,  'پیکان::Peykan':9, 'پژو ۲۰۶\u200d::Peugeot 206':10,  'پراید صندوق\u200cدار::Pride':11, 'ام\u200cوی\u200cام::MVM':12,
  'زانتیا::Citroen Xantia':13, 'مزدا::Mazda':14,  'کیا::Kia':15, 'رنو::Renault':16, 'سمند::Samand':17, 'تندر ۹۰::Tondar 90':18,
   'تویوتا::Toyota':19,  'هیوندای (غیره)::Hyundai':20, 'رانا::Runna':21, 'هیوندای آزرا::Hyundai Azera':22, 'بی\u200cام\u200cو::BMW':23,
    'هیوندای سوناتا::Hyundai Sonata':24,  'بنز::Mercedes-Benz':25, 'دوو::Daewoo':26,
                                      'گل::Gol' :27, 'پژو روآ / آر\u200cدی::RD/ROA':28})

dffffv["brand"] = dffffv["brand"].replace(to_replace ="وانت",
                 value =29)

dffffv["brand"] = dffffv["brand"].replace(to_replace ="سایر",
                 value =30)

dffffv["brand"]=dffffv["brand"].fillna(0)

dffffv['brand'] = dffffv['brand'].astype(int)

dffffv.shape

dffffv["body_status"] = dffffv["body_status"].replace({'witout-color': 1, 'intact':2, 'few-spots-of-color':3,  'multiple-spots-paint':4, 
                                                  'half-paint' :5, 'two-spots-of-color':6, 'one-spot-paint':7, 'two-spots-paint':8, 'some-scratches':9,
                                                    'full-paint':10, 'one-spot-of-color':11,  'by-accident':12, 'out':13,'paintless-dent-removal':14,
                                              'accidental'     :15, 'junk':16})

dffffv["body_status"]=dffffv["body_status"].fillna(0)

dffffv['body_status'] = dffffv['body_status'].astype(int)

dffffv.shape

dffffv=dffffv.drop(["post_data","post_type","category","document","usage"],axis=1)

dffffv.shape

dffffv["gearbox"]=dffffv["gearbox"].fillna(0)

dffffv["gearbox"] = dffffv["gearbox"].replace({"automatic": 0,"manual":1})

dffffv.shape

dffffv=dffffv.drop("brand_model",axis=1)

dffffv.shape

import re # for regex
def clean(text):
    cleaned = re.compile(r'\n')
    return re.sub(cleaned,'',text)

dffffv.description = dffffv.description.apply(clean)

dffffv.shape

def is_special(text):
    rem = ''
    for i in text:
        if i.isalnum():
            rem = rem + i
        else:
            rem = rem + ' '
    return rem

dffffv.description = dffffv.description.apply(is_special)

dffffv.shape

import re # for regex
def clean5(text):
    cleaned = re.compile(r'\\u')
    return re.sub(cleaned,'',text)

dffffv.description = dffffv.description.apply(clean5)

dffffv.shape

dffffv['text'] = dffffv['title'] + " "+ dffffv['description']

dffffv.shape

def set_types(df):
    df.title = df.title.astype(str)
    df.description = df.description.astype(str)
    return df

dffffv.shape

dffffv=set_types(dffffv)

dffffv.shape

def replace_nan(entry):
    if entry == 'nan':
        return '#'
    return entry
dffffv.title = dffffv.title.apply(replace_nan)
dffffv.description = dffffv.description.apply(replace_nan)

dffffv.shape

# dffffv.drop_duplicates(subset =['description', 'title'], inplace = True)

dffffv.shape

lemma = InformalLemmatizer()

# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b
def remove_emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)

# https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py  emoticons list
# https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt Chat shortcuts

def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r' ', text)

def remove_punctuations(text):
    punctuations = re.compile(r'[~`!@#$%^&*(,<،>){}\\/|\'"?؟_+-=~\[\]]')
    return punctuations.sub(r' ', text)

def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r' ', text)

def remove_weird_chars(text):
    weridPatterns = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u'\U00010000-\U0010ffff'
                               u"\u200d"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\u3030"
                               u"\ufe0f"
                               u"\u2069"
                               u"\u2066"
                               u"\u200c"
                               u"\u2068"
                               u"\u2067"
                               "]+", flags=re.UNICODE)
    patterns = [re.compile('\r'), re.compile('\n'), re.compile('&amp;')]
    text = weridPatterns.sub(r'', text)
    for p in patterns:
        text = p.sub(r' ', text)
    return text

def remove_extra_repeated_alpha(text):
    """
    Remove extra repeated alphabets in a word
    check these links:
    demo : https://regex101.com/r/ALxocA/1
    Question: https://bit.ly/2DoiPqS
    """
    return re.sub(r'([^\W\d_])\1{2,}', r'\1', text)


def clean_up(text, url=True, html=True, weird_patterns=True , lemmatize=False, 
               stopwords=True, isalpha=False, punctuations=True, remove_extra_alpha=True):
    # remove url
    if url:
        text = remove_urls(text)
    # remove html tags
    if html:
        text = remove_html(text)
    # remove emokis / symbols & pictographs / transport & map symbols / flags (iOS)
    if weird_patterns:
        text = remove_weird_chars(text)
    # remove punctuations
    if punctuations:
        text = remove_punctuations(text)
    # Alter words with repeated alphabets
    if remove_extra_repeated_alpha:
        text = remove_extra_repeated_alpha(text)
    # tokenize text
    tokens = word_tokenize(text)
    # remove stop words
    if stopwords:
        tokens = [word for word in tokens if word not in stopwords_list()]
    # remove non-alphabetic items
    if isalpha:
        tokens = [word for word in tokens if word.isalpha()]
    # lemmatize words
    if lemmatize:
        tokens = [lemma.lemmatize(word) for word in tokens]
    text = ' '.join(tokens)
    
    return text

dffffv = set_types(dffffv)
dffffv.text = dffffv.text.apply(str)

dffffv.shape

all_words_title = ' '.join([text for text in dffffv.title])
all_words = arabic_reshaper.reshape(all_words_title)

dffffv.shape

dffffv=dffffv.drop('options',axis=1)

dffffv=dffffv.drop('new_price',axis=1)

dffffv.to_csv('dffffv.csv', index=False)

pd.read_csv('dffffv.csv')

from google.colab import drive
drive.mount('drive')

!cp dffffv.csv "drive/My Drive/"

